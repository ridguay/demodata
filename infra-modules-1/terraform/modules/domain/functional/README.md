<!-- BEGIN_TF_DOCS -->
## Requirements

| Name | Version |
|------|---------|
| <a name="requirement_terraform"></a> [terraform](#requirement\_terraform) | ~> 1.3.0 |
| <a name="requirement_azapi"></a> [azapi](#requirement\_azapi) | ~> 1.15.0 |
| <a name="requirement_azurerm"></a> [azurerm](#requirement\_azurerm) | ~> 3.114.0 |
| <a name="requirement_databricks"></a> [databricks](#requirement\_databricks) | ~> 1.51.0 |
| <a name="requirement_template"></a> [template](#requirement\_template) | ~> 2.2.0 |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_adf_diagnostics_vars"></a> [adf\_diagnostics\_vars](#input\_adf\_diagnostics\_vars) | n/a | <pre>object({<br>    diagnostic_setting_name        = optional(string, "NNDAP_log_analytics")<br>    log_analytics_workspace_id     = string<br>    log_analytics_destination_type = optional(string, null)<br>    enabled_logs                   = optional(list(string), [])<br>    metrics = optional(list(object({<br>      category = string<br>      enabled  = bool<br>    })), [])<br>  })</pre> | `null` | no |
| <a name="input_adf_infra_components_vars"></a> [adf\_infra\_components\_vars](#input\_adf\_infra\_components\_vars) | n/a | <pre>object({<br>    # Databricks Linked Service<br>    databricks_workspace_url = string<br>    databricks_workspace_id  = string<br>    pool_id                  = optional(string, null)<br>    cluster_spark_version    = optional(string, "10.4.x-scala2.12")<br><br>    # Key Vault Linked Service Variables<br>    key_vault_id = string<br><br>    # Run SQL Query Pipeline Variables<br>    notebook_path = string<br><br>    # Start Stop VM Pipeline Variables<br>    subscription_id = string<br>    # virtual_machine_name = string<br><br>    # Storage Connection Variables<br>    storage_account_primary_dfs_endpoint = string<br>    dataset_config = list(object({<br>      dataset_name      = string<br>      container_name    = string<br>      file_type         = string<br>      compression_codec = string<br>      level             = optional(string)<br>    }))<br>  })</pre> | `null` | no |
| <a name="input_adf_role_assignments_vars"></a> [adf\_role\_assignments\_vars](#input\_adf\_role\_assignments\_vars) | ## ROLE ASSIGNMENTS ### | <pre>object({<br>    user_principal_roles = optional(map(list(string)), {})<br>    principal_id_roles   = optional(map(list(string)), {})<br>  })</pre> | `null` | no |
| <a name="input_artifactory_password_secret_name"></a> [artifactory\_password\_secret\_name](#input\_artifactory\_password\_secret\_name) | The name of the Artifactory secret inside the meta key vault. This will not yet be used but in the future will be required for the Databricks init script. | `string` | n/a | yes |
| <a name="input_data_factory_vars"></a> [data\_factory\_vars](#input\_data\_factory\_vars) | ## Data Factory Specific Vars ### | <pre>list(object({<br>    # Data Factory Instance<br>    name                   = string<br>    public_network_enabled = optional(bool, false)<br>    global_parameters = optional(list(object({<br>      name  = string<br>      type  = string<br>      value = string<br>    })), [])<br>    identity_type              = optional(string, "SystemAssigned")<br>    identity_ids               = optional(list(string), [])<br>    private_endpoint_subnet_id = string<br>    private_endpoint_ip        = string<br>    git_sync_configuration = optional(object({<br>      account_name       = string<br>      branch_name        = string<br>      project_name       = string<br>      repository_name    = string<br>      root_folder        = string<br>      tenant_id          = string<br>      publishing_enabled = bool<br>    }), null)<br><br>    # Key Vault SHIR<br>    key_vault_name                                   = string<br>    key_vault_sku_name                               = optional(string, "standard")<br>    key_vault_tenant_id                              = string<br>    key_vault_soft_delete_retention_days             = optional(number, 90)<br>    key_vault_enable_purge_protection                = optional(bool, true)<br>    key_vault_enable_rbac_authorization              = optional(bool, false)<br>    key_vault_enable_disk_encryption                 = optional(bool, false)<br>    key_vault_network_control_allowed_subnet_ids     = optional(list(string), null)<br>    key_vault_network_control_allowed_ip_rules       = optional(list(string), null)<br>    key_vault_network_control_allowed_azure_services = optional(bool, false)<br>    key_vault_private_endpoint_subnet_id             = optional(string, "")<br>    key_vault_create_private_endpoint                = optional(bool, true)<br><br>    # Self-Hosted Integration Runtime<br>    shir_name = string<br><br>    # Azure Hosted Integration Runtime<br>    storage_account_id             = optional(string, null)<br>    managed_private_endpoint_name  = optional(string, "")<br>    azure_integration_runtime_name = optional(string, "")<br>  }))</pre> | `[]` | no |
| <a name="input_databricks_diagnostics_vars"></a> [databricks\_diagnostics\_vars](#input\_databricks\_diagnostics\_vars) | n/a | <pre>object({<br>    diagnostic_setting_name        = optional(string, "NNDAP_log_analytics")<br>    target_resource_id             = string<br>    log_analytics_workspace_id     = string<br>    log_analytics_destination_type = optional(string, null)<br>    enabled_logs                   = optional(list(string), [])<br>    metrics = optional(list(object({<br>      category = string<br>      enabled  = bool<br>    })), [])<br>  })</pre> | `null` | no |
| <a name="input_databricks_functional_vars"></a> [databricks\_functional\_vars](#input\_databricks\_functional\_vars) | n/a | <pre>object({<br>    # Workspace Config<br>    workspace_config_nn_pypi_index_url          = optional(string, "https://artifactory.insim.biz/artifactory/api/pypi/nn-pypi/simple")    # If you run into authentication issues, please the same token for both indices. It's likely that the packaged pip version is out of date. Multi-credential support is available as of pip 21.2 https://github.com/pypa/pip/pull/10033<br>    workspace_config_nn_customer_pypi_index_url = optional(string, "https://artifactory.insim.biz/artifactory/api/pypi/nndap-pypi/simple") # If you run into authentication issues, please use the same token for both indices. It's likely that the packaged pip version is out of date. Multi-credential support is available as of pip 21.2 https://github.com/pypa/pip/pull/10033<br>    workspace_config_kafka_secret_name          = string<br><br>    # UC enabled<br>    cluster_uc_enabled = optional(bool, true)<br><br>    # Legacy Clusters enabled<br>    legacy_clusters_enabled = optional(bool, true)<br><br>    # Databricks Asset Bundles<br>    workspace_url = optional(string, "")<br>    env_domain    = optional(string, "")<br><br>    # Notebook Mounting Secrets<br>    notebook_mounting_secret_scope = string<br>    notebook_mounting_secrets      = map(string)<br><br>    # Variables for Legacy Clusters<br>    legacy_cluster_names               = optional(list(string), [])<br>    legacy_user_specific_cluster_names = optional(list(string), [])<br><br>    # Variables for Primary Cluster(s)<br>    cluster_names = list(string)<br>    cluster_config = object({<br>      legacy_spark_version_id     = optional(string, "12.2.x-scala2.12")<br>      spark_version_id            = optional(string, "15.4.x-scala2.12")<br>      runtime_engine              = optional(string, "PHOTON")<br>      node_type_id                = optional(string, "Standard_D4ads_v5")<br>      autoterminate_after_minutes = optional(number, 120)<br>      tags                        = optional(map(string), {})<br>      minimum_workers             = optional(number, 2)<br>      maximum_workers             = optional(number, 8)<br>      extra_spark_configuration   = optional(map(string), {})<br>    })<br>    cluster_pypi_packages = optional(list(string), [])<br><br>    cluster_unity_catalog_volume_name                   = string<br>    cluster_unity_catalog_volume_storage_account_name   = string<br>    cluster_unity_catalog_volume_storage_container_name = string<br><br>    # Unity catalog volume / schema Entra group name, service principal name<br>    unity_catalog_group_name = string<br>    unity_catalog_sp_name    = string<br><br>    # Variables for User-specific Cluster(s)<br>    user_specific_cluster_names = list(string)<br>    user_specific_cluster_config = object({<br>      spark_version_id            = optional(string, "15.4.x-scala2.12")<br>      node_type_id                = optional(string, "Standard_DS3_v2")<br>      autoterminate_after_minutes = optional(number, 30)<br>      tags                        = optional(map(string), {})<br>      minimum_workers             = optional(number, 1)<br>      maximum_workers             = optional(number, 2)<br>      extra_spark_configuration   = optional(map(string), {})<br>    })<br><br>    # Service Principal IAM<br>    service_principal_configuration_iam = object({<br>      name           = string<br>      application_id = string<br>    })<br>  })</pre> | `null` | no |
| <a name="input_databricks_role_assignments_vars"></a> [databricks\_role\_assignments\_vars](#input\_databricks\_role\_assignments\_vars) | n/a | <pre>object({<br>    resource_ids         = list(string)<br>    user_principal_roles = optional(map(list(string)), {})<br>    principal_id_roles   = optional(map(list(string)), {})<br>  })</pre> | `null` | no |
| <a name="input_disk_encryption_diagnostics_vars"></a> [disk\_encryption\_diagnostics\_vars](#input\_disk\_encryption\_diagnostics\_vars) | n/a | <pre>object({<br>    diagnostic_setting_name        = optional(string, "NNDAP_log_analytics")<br>    log_analytics_workspace_id     = string<br>    log_analytics_destination_type = optional(string, null)<br>    enabled_logs                   = optional(list(string), [])<br>    metrics = optional(list(object({<br>      category = string<br>      enabled  = bool<br>    })), [])<br>  })</pre> | `null` | no |
| <a name="input_interface_storage_role_assignments_vars"></a> [interface\_storage\_role\_assignments\_vars](#input\_interface\_storage\_role\_assignments\_vars) | n/a | <pre>object({<br>    resource_ids         = list(string)<br>    user_principal_roles = optional(map(list(string)), {})<br>    principal_id_roles   = optional(map(list(string)), {})<br>  })</pre> | `null` | no |
| <a name="input_key_vault_vars"></a> [key\_vault\_vars](#input\_key\_vault\_vars) | # Key Vault Specific Vars ### | <pre>object({<br>    name                                   = string<br>    sku_name                               = optional(string, "standard")<br>    tenant_id                              = string<br>    soft_delete_retention_days             = optional(number, 90)<br>    enable_purge_protection                = optional(bool, true)<br>    enable_rbac_authorization              = optional(bool, false)<br>    enable_disk_encryption                 = optional(bool, false)<br>    network_control_allowed_subnet_ids     = optional(list(string), null)<br>    network_control_allowed_ip_rules       = optional(list(string), null)<br>    network_control_allowed_azure_services = optional(bool, false)<br>    private_endpoint_subnet_id             = optional(string, "")<br>    create_private_endpoint                = optional(bool, true)<br>    key_vault_private_endpoint_ip          = optional(string, "")<br>  })</pre> | `null` | no |
| <a name="input_kv_infra_diagnostics_vars"></a> [kv\_infra\_diagnostics\_vars](#input\_kv\_infra\_diagnostics\_vars) | n/a | <pre>object({<br>    diagnostic_setting_name        = optional(string, "NNDAP_log_analytics")<br>    log_analytics_workspace_id     = string<br>    log_analytics_destination_type = optional(string, null)<br>    enabled_logs                   = optional(list(string), [])<br>    metrics = optional(list(object({<br>      category = string<br>      enabled  = bool<br>    })), [])<br>  })</pre> | `null` | no |
| <a name="input_kv_infra_role_assignments_vars"></a> [kv\_infra\_role\_assignments\_vars](#input\_kv\_infra\_role\_assignments\_vars) | n/a | <pre>object({<br>    user_principal_roles = optional(map(list(string)), {})<br>    principal_id_roles   = optional(map(list(string)), {})<br>  })</pre> | `null` | no |
| <a name="input_kv_sources_diagnostics_vars"></a> [kv\_sources\_diagnostics\_vars](#input\_kv\_sources\_diagnostics\_vars) | n/a | <pre>object({<br>    diagnostic_setting_name        = optional(string, "NNDAP_log_analytics")<br>    target_resource_id             = string<br>    log_analytics_workspace_id     = string<br>    log_analytics_destination_type = optional(string, null)<br>    enabled_logs                   = optional(list(string), [])<br>    metrics = optional(list(object({<br>      category = string<br>      enabled  = bool<br>    })), [])<br>  })</pre> | `null` | no |
| <a name="input_kv_sources_role_assignments_vars"></a> [kv\_sources\_role\_assignments\_vars](#input\_kv\_sources\_role\_assignments\_vars) | n/a | <pre>object({<br>    resource_ids         = list(string)<br>    user_principal_roles = optional(map(list(string)), {})<br>    principal_id_roles   = optional(map(list(string)), {})<br>  })</pre> | `null` | no |
| <a name="input_location"></a> [location](#input\_location) | The location to deploy all resources to | `string` | `"West Europe"` | no |
| <a name="input_meta_key_vault_name"></a> [meta\_key\_vault\_name](#input\_meta\_key\_vault\_name) | The name of the meta key vault | `string` | n/a | yes |
| <a name="input_meta_resource_group"></a> [meta\_resource\_group](#input\_meta\_resource\_group) | The name of the meta resource group | `string` | n/a | yes |
| <a name="input_resource_group_name"></a> [resource\_group\_name](#input\_resource\_group\_name) | The Resource Group name | `string` | n/a | yes |
| <a name="input_storage_diagnostics_vars"></a> [storage\_diagnostics\_vars](#input\_storage\_diagnostics\_vars) | n/a | <pre>object({<br>    diagnostic_setting_name        = optional(string, "NNDAP_log_analytics")<br>    storage_account_id             = string<br>    log_analytics_workspace_id     = string<br>    log_analytics_destination_type = optional(string, null)<br>    enabled_logs                   = optional(list(string), [])<br>    metrics = optional(list(object({<br>      category = string<br>      enabled  = bool<br>    })), [])<br>  })</pre> | `null` | no |
| <a name="input_storage_role_assignments_vars"></a> [storage\_role\_assignments\_vars](#input\_storage\_role\_assignments\_vars) | n/a | <pre>object({<br>    resource_ids         = list(string)<br>    user_principal_roles = optional(map(list(string)), {})<br>    principal_id_roles   = optional(map(list(string)), {})<br>  })</pre> | `null` | no |
| <a name="input_tags"></a> [tags](#input\_tags) | Tags to assign to the resources | `map(string)` | `{}` | no |
| <a name="input_version_vars"></a> [version\_vars](#input\_version\_vars) | n/a | <pre>object({<br>    storage_account_name   = string<br>    storage_container_name = string<br>    blob_folder            = string<br>    infra_modules_version  = string<br>  })</pre> | n/a | yes |
| <a name="input_virtual_machine_vars"></a> [virtual\_machine\_vars](#input\_virtual\_machine\_vars) | n/a | <pre>list(object({<br>    # Virtual Machine<br>    virtual_machine_name_prefix                  = string<br>    virtual_machine_private_ip_addresses         = list(string)<br>    virtual_machine_size                         = optional(string, "Standard_D4_v4")<br>    virtual_machine_os_disk_storage_account_type = optional(string, "StandardSSD_LRS")<br>    virtual_machine_os_disk_size_gb              = optional(number, 127)<br>    virtual_machine_timezone                     = optional(string, "W. Europe Standard Time")<br>    virtual_machine_subnet_id                    = string<br>    virtual_machine_accelerated_networking       = optional(bool, false)<br>    virtual_machine_application_ids              = optional(list(string), [])<br><br>    # VM Extensions and Applications<br>    monitoring_agent_config = object({<br>      log_analytics_workspace_workspace_id = string<br>      log_analytics_workspace_auth_key     = string<br>    })<br>    diagnostics_storage_account_name = optional(string, null)<br>    law_data_collection_rule_id      = optional(string, null)<br>    vm_patch = object({<br>      schedule_name = string #"Name of the schedule"<br>      start_time    = string #"Start time of the schedule"<br>      start_date    = string #"Start date of the schedule"<br>      recur_every   = string #"Recurrence interval"<br>      duration      = string #"Duration of the patching"<br>    })<br>    # virtual_machine_application_ids = list(string)<br>  }))</pre> | `null` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_data_factory__id"></a> [data\_factory\_\_id](#output\_data\_factory\_\_id) | ID of the Data Factory instance |
| <a name="output_data_factory__shir_name"></a> [data\_factory\_\_shir\_name](#output\_data\_factory\_\_shir\_name) | The name of the Self-Hosted Integration Runtime |
| <a name="output_databricks__cluster_ids"></a> [databricks\_\_cluster\_ids](#output\_databricks\_\_cluster\_ids) | IDs of the (main) clusters |
| <a name="output_databricks__user_specific_cluster_ids"></a> [databricks\_\_user\_specific\_cluster\_ids](#output\_databricks\_\_user\_specific\_cluster\_ids) | IDs of the user specific clusters |
<!-- END_TF_DOCS -->